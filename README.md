# EmoFace: Audio-driven Emotional 3D Face Animation

PyTorch implementation for the paper **EmoFace: Audio-driven Emotional 3D Face Animation**

### Environment

- Linux
- Python 3.7.0
- Pytorch 1.9.0
- CUDA 11.3

### Data

As we could not publish the full dataset yet, we provide one piece of data from the evaluation set. 

- **ACTOR**: Original performance video of the actor.
- **VALID_CTR**: Extracted controller value sequence.
- **VIDEO**: MetaHuman Animation generated by **VALID_CTR**
- **WAV**: Audio clip of the corresponding video.

### Training and Testing

### Demo

### Visualization

Output of the model is *.txt* files containing controller values, each row stands for one frame. To visualize the output, you need a MetaHuman model with all the controller rigs in the [valid_attr_names.txt](https://github.com/SJTU-Lucy/EmoFace/blob/main/dataset/valid_attr_names.txt).